{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run fullchain tests\n",
    "\n",
    "See: finnet-pipeline/docker-tests/fullchain/run_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add packages, append to `dags/requirements_py3.txt` and run `!pip3 install -r /usr/local/dags/requirements_py3.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r /usr/local/dags/requirements_py3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "### Stop current SC, test assumes no existing SC\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GRAPH_DB\"] = \"\"\"bolt://neo4j:test@neo4j:7687\"\"\"\n",
    "os.environ[\"NEO4J_SSH_PORT\"] = \"22\"\n",
    "os.environ[\"NEO4J_SSH_USERNAME\"] = \"root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PIPELINE_DATA_PATH'] = \"/datasets/finnet\"\n",
    "os.environ['PIPELINE_DATA_FORMAT'] = \"parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/dags\")\n",
    "\n",
    "from run_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the list of tasks to test\n",
    "dolist = [\n",
    "    'build_lists', 'resolve_entities',\n",
    "    'neo4j_purger', 'neo4j_writer',\n",
    "    'graph_tools'\n",
    "]\n",
    "\n",
    "# Get neo4j ssh username and port\n",
    "neo4j_ssh_username = os.environ.get('NEO4J_SSH_USERNAME', 'neo4j')\n",
    "neo4j_ssh_port = int(os.environ.get('NEO4J_SSH_PORT', 9000))\n",
    "\n",
    "# Setup the spark configuration\n",
    "config = dict()\n",
    "config['SparkConfiguration'] = (SparkConf()\n",
    "                                .setMaster('local[*]')\n",
    "                                .setAppName(\"test create data\")\n",
    "                                .set(\"spark.executor.memory\", \"1024m\"))\n",
    "\n",
    "# Get the graph specs\n",
    "datalist = os.listdir(LOCAL_DATA_PATH)\n",
    "jsonlist = [k for k in datalist if re.match(r'.*\\.json$', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1 json\n",
    "gspec = jsonlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph spec\n",
    "with open(os.path.join(LOCAL_DATA_PATH, gspec), 'r') as f:\n",
    "    graph_spec = GraphSpec.from_dict(json.load(f))\n",
    "    spec = graph_spec.to_dict()\n",
    "\n",
    "tables_path = os.path.join(DATA_PATH, graph_spec.name, 'tables')\n",
    "n_path = os.path.join(DATA_PATH, graph_spec.name, 'node_list')\n",
    "e_path = os.path.join(DATA_PATH, graph_spec.name, 'edge_list')\n",
    "n_path_res = os.path.join(DATA_PATH, graph_spec.name, 'node_list_resolved')\n",
    "e_path_res = os.path.join(DATA_PATH, graph_spec.name, 'edge_list_resolved')\n",
    "\n",
    "logging.info(\"Processing \" + gspec)\n",
    "\n",
    "# Use graph specification's neo4j connection\n",
    "neo_config = {\n",
    "    'uri': spec['graph_uri'],\n",
    "    'max_retries': config.get('neo4j.max_retries', 5),\n",
    "    'max_batchsize': config.get('neo4j.max_batchsize', 10000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'connection': 'data_uri_value',\n",
       " 'poll_frequency': '0 2 * * *',\n",
       " 'tables': {('test_data_chocolate_node_list',\n",
       "   'fn_test_data_chocolate_node_list'): {('id', 'fn_id')},\n",
       "  ('test_data_sweets_node_list',\n",
       "   'fn_test_data_sweets_node_list'): {('id', 'fn_id'), ('prop', 'fn_prop')},\n",
       "  ('test_data_toffee_node_list',\n",
       "   'fn_test_data_toffee_node_list'): {('hide', 'fn_hide'),\n",
       "   ('id', 'fn_id'),\n",
       "   ('prop', 'fn_prop')},\n",
       "  ('test_data_chocolate_edge_list',\n",
       "   'fn_test_data_chocolate_edge_list'): {('chocolate_s',\n",
       "    'fn_chocolate_s'), ('chocolate_t', 'fn_chocolate_t')},\n",
       "  ('test_data_sweets_edge_list',\n",
       "   'fn_test_data_sweets_edge_list'): {('sweets_s', 'fn_sweets_s'), ('sweets_t',\n",
       "    'fn_sweets_t')},\n",
       "  ('test_data_toffee_edge_list',\n",
       "   'fn_test_data_toffee_edge_list'): {('toffee_s', 'fn_toffee_s'), ('toffee_t',\n",
       "    'fn_toffee_t')}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_spec.table_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list\n",
    "if 'build_lists' in dolist:\n",
    "    logging.info(\"Building lists...\")\n",
    "    build_node_lists(\n",
    "        graph_specification=graph_spec,\n",
    "        spark_config=(SparkConfFactory()\n",
    "                      .set_master('local[*]')\n",
    "                      .set_app_name('test create data')\n",
    "                      .set('spark.executor.memory', '1g')),\n",
    "        tables_path=tables_path,\n",
    "        node_path=n_path,\n",
    "        data_format=DATA_FORMAT,\n",
    "    )\n",
    "    build_edge_lists(\n",
    "        graph_specification=graph_spec,\n",
    "        spark_config=(SparkConfFactory()\n",
    "                      .set_master('local[*]')\n",
    "                      .set_app_name('test create data')\n",
    "                      .set('spark.executor.memory', '1g')),\n",
    "        tables_path=tables_path,\n",
    "        edge_path=e_path,\n",
    "        data_format=DATA_FORMAT,\n",
    "    )\n",
    "    logging.info(\"Checking build_lists...\")\n",
    "    with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "        sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "        assert test_build_lists(spark_ctx, sql_context, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|fn_id|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reads only needed columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "    sql_context.read.parquet(\"/datasets/finnet/test_data/node_list/fn_chocolate_nodes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|fn_toffee_s|fn_toffee_t|\n",
      "+-----------+-----------+\n",
      "|          1|          0|\n",
      "|          1|          9|\n",
      "|          2|          3|\n",
      "|          2|          4|\n",
      "|          3|          4|\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reads only needed columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "    sql_context.read.parquet(\"/datasets/finnet/test_data/edge_list/fn_toffee_relations\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve entities\n",
    "if 'resolve_entities' in dolist:\n",
    "    logging.info(\"Resolving entities...\")\n",
    "    resolve_node_entities(\n",
    "        graph_specification=graph_spec,\n",
    "        spark_config=(SparkConfFactory()\n",
    "                      .set_master('local[*]')\n",
    "                      .set_app_name('test create data')\n",
    "                      .set('spark.executor.memory', '1g')),\n",
    "        entity_maps=dict(),\n",
    "        input_node_path=n_path,\n",
    "        output_node_path=n_path_res,\n",
    "        output_node_id='_canonical_id',\n",
    "        data_format=DATA_FORMAT\n",
    "    )\n",
    "    resolve_edge_entities(\n",
    "        graph_specification=graph_spec,\n",
    "        spark_config=(SparkConfFactory()\n",
    "                      .set_master('local[*]')\n",
    "                      .set_app_name('test create data')\n",
    "                      .set('spark.executor.memory', '1g')),\n",
    "        entity_maps=dict(),\n",
    "        input_edge_path=e_path,\n",
    "        output_edge_path=e_path_res,\n",
    "        output_edge_source_id='_canonical_id_source',\n",
    "        output_edge_target_id='_canonical_id_target',\n",
    "        data_format=DATA_FORMAT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|        fn_id|_canonical_id|\n",
      "+-------------+-------------+\n",
      "|illegal ΁code|illegal ΁code|\n",
      "|     test ΄id|     test ΄id|\n",
      "|           11|           11|\n",
      "|           29|           29|\n",
      "|           30|           30|\n",
      "+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Produces _canonical_id columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "    sql_context.read.parquet(\"/datasets/finnet/test_data/node_list_resolved/fn_chocolate_nodes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+\n",
      "|fn_toffee_s|fn_toffee_t|_canonical_id_source|_canonical_id_target|\n",
      "+-----------+-----------+--------------------+--------------------+\n",
      "|          8|          9|                   8|                   9|\n",
      "|          1|          9|                   1|                   9|\n",
      "|          4|          9|                   4|                   9|\n",
      "|          3|          4|                   3|                   4|\n",
      "|          2|          4|                   2|                   4|\n",
      "+-----------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Produces _canonical_id columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "    sql_context.read.parquet(\"/datasets/finnet/test_data/edge_list_resolved/fn_toffee_relations\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from-24e2d3dc-2624-4cfe-b7b2-3c1fabb33e01\n",
      "to-d11d114b-f9f5-48c7-86b1-0496ee75462e\n",
      "+-----------------------------------------+---------------------------------------+\n",
      "|from-24e2d3dc-2624-4cfe-b7b2-3c1fabb33e01|to-d11d114b-f9f5-48c7-86b1-0496ee75462e|\n",
      "+-----------------------------------------+---------------------------------------+\n",
      "+-----------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What does EntityMapper do???\n",
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    from fncore.tasks.resolve_entities import EntityMapper\n",
    "    \n",
    "    entityMapper = EntityMapper(spark_ctx, {})\n",
    "    # Think these are just placeholder column names\n",
    "    print(entityMapper._from)\n",
    "    print(entityMapper._to)\n",
    "    entityMapper._map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like EntityMapper isn't doing anything here,\n",
    "# since entity_map param is empty, _canonical_id just takes the id from index_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purge existing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purging the graph\n",
    "if 'neo4j_purger' in dolist:\n",
    "    logging.info(\"Purging Neo4j...\")\n",
    "    neo4j_manager.purge(graph_spec,\n",
    "                        username=neo4j_ssh_username,\n",
    "                        port=neo4j_ssh_port)\n",
    "    logging.info(\"Checking purging neo4j...\")\n",
    "    with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "        assert test_neo4j_purger(neo_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 0}]\n"
     ]
    }
   ],
   "source": [
    "# Check nodes in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\"MATCH (n) RETURN count(n) as count\")\n",
    "    print(cursor.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write nodes and edges to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote temp nodes .csv to /tmp/tmprr5ky4i4/tmpst1kdln_.csv\n",
      "Wrote temp edges .csv to /tmp/tmprr5ky4i4/tmpjnxkc4sj.csv\n",
      "Wrote temp edges .csv to /tmp/tmprr5ky4i4/tmp0jn7efje.csv\n",
      "Wrote temp edges .csv to /tmp/tmprr5ky4i4/tmpwpuz9zp7.csv\n"
     ]
    }
   ],
   "source": [
    "# Graph writer\n",
    "if 'neo4j_writer' in dolist:\n",
    "    logging.info(\"Writing to Neo4j...\")\n",
    "        \n",
    "    graph_to_neo4j.graph_to_neo4j(graph_specification=graph_spec,\n",
    "                                  spark_config=SparkConfFactory()\n",
    "                                  .set_master('local[*]')\n",
    "                                  .set_app_name('write neo4j nodes')\n",
    "                                  .set(\"spark.driver.maxResultSize\",\n",
    "                                       \"1g\")\n",
    "                                  .set('spark.executor.memory',\n",
    "                                       '1g'),\n",
    "                                  input_node_path=n_path_res,\n",
    "                                  input_edge_path=e_path_res,\n",
    "                                  username=neo4j_ssh_username,\n",
    "                                  port=neo4j_ssh_port,\n",
    "                                  debug_write=True,\n",
    "                                  verbose=True\n",
    "                                  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - hadoop supergroup          0 2019-12-14 07:02 /datasets/finnet/test_data/node_list_resolved/combined_nodes\r\n",
      "drwxr-xr-x   - hadoop supergroup          0 2019-12-14 10:12 /datasets/finnet/test_data/node_list_resolved/fn_chocolate_nodes\r\n",
      "drwxr-xr-x   - hadoop supergroup          0 2019-12-14 10:12 /datasets/finnet/test_data/node_list_resolved/fn_sweets_nodes\r\n",
      "drwxr-xr-x   - hadoop supergroup          0 2019-12-14 10:12 /datasets/finnet/test_data/node_list_resolved/fn_toffee_nodes\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasets/finnet/test_data/node_list_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect with py2neo\n",
    "from py2neo import Graph\n",
    "graph = Graph(\"bolt://neo4j:test@neo4j:7687\", user=\"neo4j\", password=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var link = document.createElement(\"link\");\n",
       "\tlink.ref = \"stylesheet\";\n",
       "\tlink.type = \"text/css\";\n",
       "\tlink.href = \"https://cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.css\";\n",
       "\tdocument.head.appendChild(link);\n",
       "require.config({     paths: {         vis: '//cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.min'     } }); require(['vis'], function(vis) {  window.vis = vis; }); "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"e00a60a4-a11c-48fd-9c55-d3bf5dc72e2b\" style=\"height: 400px;\"></div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "    var nodes = [{\"id\": 23, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '4', '_label': '5;4'}\"}, {\"id\": 27, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '9', '_label': '9;0'}\"}, {\"id\": 20, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '18', '_label': '18'}\"}, {\"id\": 28, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '24'}\"}, {\"id\": 14, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '23'}\"}, {\"id\": 30, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '7', '_label': '2;7'}\"}, {\"id\": 0, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '16', '_label': '16'}\"}, {\"id\": 33, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '6', '_label': '3;6'}\"}, {\"id\": 38, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '21'}\"}, {\"id\": 7, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '8', '_label': '1;8'}\"}, {\"id\": 5, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': 'illegal \\\\u0381code'}\"}, {\"id\": 22, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '25'}\"}, {\"id\": 11, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '13', '_label': '13'}\"}, {\"id\": 1, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '35'}\"}, {\"id\": 26, \"label\": \"\", \"group\": \"sweets\", \"title\": \"{'_canonical_id': '10', '_label': '10'}\"}, {\"id\": 16, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '32'}\"}, {\"id\": 13, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '26'}\"}, {\"id\": 31, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '11', '_label': '11'}\"}, {\"id\": 4, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '12', '_label': '12'}\"}, {\"id\": 32, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '0', '_label': '9;0'}\"}, {\"id\": 2, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '20'}\"}, {\"id\": 10, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '39'}\"}, {\"id\": 41, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '5', '_label': '5;4'}\"}, {\"id\": 15, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '1', '_label': '1;8'}\"}, {\"id\": 9, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '27'}\"}, {\"id\": 17, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '2', '_label': '2;7'}\"}, {\"id\": 21, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '19', '_label': '19'}\"}, {\"id\": 6, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '3', '_label': '3;6'}\"}, {\"id\": 37, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '34'}\"}, {\"id\": 39, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '14', '_label': '14'}\"}, {\"id\": 24, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '29'}\"}, {\"id\": 25, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '17', '_label': '17'}\"}, {\"id\": 3, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '36'}\"}, {\"id\": 40, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': 'test \\u0384id'}\"}, {\"id\": 35, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '37'}\"}, {\"id\": 12, \"label\": \"\", \"group\": \"sweets\", \"title\": \"{'_canonical_id': '15', '_label': '15'}\"}, {\"id\": 34, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '33'}\"}, {\"id\": 19, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '28'}\"}, {\"id\": 36, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '38'}\"}, {\"id\": 8, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '31'}\"}, {\"id\": 18, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '22'}\"}, {\"id\": 29, \"label\": \"\", \"group\": \"chocolate\", \"title\": \"{'_canonical_id': '30'}\"}];\n",
       "    var edges = [{\"from\": 23, \"to\": 27, \"label\": \"toffee\"}, {\"from\": 23, \"to\": 20, \"label\": \"chocolate\"}, {\"from\": 23, \"to\": 28, \"label\": \"chocolate\"}, {\"from\": 23, \"to\": 14, \"label\": \"chocolate\"}, {\"from\": 23, \"to\": 30, \"label\": \"sweets\"}, {\"from\": 23, \"to\": 0, \"label\": \"sweets\"}, {\"from\": 23, \"to\": 33, \"label\": \"sweets\"}, {\"from\": 7, \"to\": 27, \"label\": \"toffee\"}, {\"from\": 7, \"to\": 5, \"label\": \"chocolate\"}, {\"from\": 22, \"to\": 11, \"label\": \"chocolate\"}, {\"from\": 22, \"to\": 1, \"label\": \"chocolate\"}, {\"from\": 26, \"to\": 33, \"label\": \"sweets\"}, {\"from\": 16, \"to\": 13, \"label\": \"chocolate\"}, {\"from\": 16, \"to\": 22, \"label\": \"chocolate\"}, {\"from\": 16, \"to\": 31, \"label\": \"chocolate\"}, {\"from\": 16, \"to\": 4, \"label\": \"chocolate\"}, {\"from\": 32, \"to\": 4, \"label\": \"sweets\"}, {\"from\": 32, \"to\": 26, \"label\": \"sweets\"}, {\"from\": 2, \"to\": 23, \"label\": \"chocolate\"}, {\"from\": 2, \"to\": 11, \"label\": \"chocolate\"}, {\"from\": 2, \"to\": 28, \"label\": \"chocolate\"}, {\"from\": 10, \"to\": 41, \"label\": \"chocolate\"}, {\"from\": 15, \"to\": 27, \"label\": \"toffee\"}, {\"from\": 15, \"to\": 32, \"label\": \"toffee\"}, {\"from\": 15, \"to\": 9, \"label\": \"chocolate\"}, {\"from\": 15, \"to\": 1, \"label\": \"chocolate\"}, {\"from\": 15, \"to\": 26, \"label\": \"sweets\"}, {\"from\": 15, \"to\": 17, \"label\": \"sweets\"}, {\"from\": 21, \"to\": 27, \"label\": \"chocolate\"}, {\"from\": 21, \"to\": 30, \"label\": \"chocolate\"}, {\"from\": 21, \"to\": 32, \"label\": \"sweets\"}, {\"from\": 21, \"to\": 6, \"label\": \"sweets\"}, {\"from\": 21, \"to\": 11, \"label\": \"sweets\"}, {\"from\": 37, \"to\": 39, \"label\": \"chocolate\"}, {\"from\": 31, \"to\": 17, \"label\": \"sweets\"}, {\"from\": 31, \"to\": 32, \"label\": \"sweets\"}, {\"from\": 11, \"to\": 27, \"label\": \"chocolate\"}, {\"from\": 11, \"to\": 24, \"label\": \"chocolate\"}, {\"from\": 11, \"to\": 20, \"label\": \"sweets\"}, {\"from\": 11, \"to\": 25, \"label\": \"sweets\"}, {\"from\": 11, \"to\": 17, \"label\": \"sweets\"}, {\"from\": 40, \"to\": 22, \"label\": \"chocolate\"}, {\"from\": 40, \"to\": 13, \"label\": \"chocolate\"}, {\"from\": 0, \"to\": 3, \"label\": \"chocolate\"}, {\"from\": 0, \"to\": 16, \"label\": \"chocolate\"}, {\"from\": 0, \"to\": 9, \"label\": \"chocolate\"}, {\"from\": 0, \"to\": 35, \"label\": \"chocolate\"}, {\"from\": 0, \"to\": 32, \"label\": \"sweets\"}, {\"from\": 0, \"to\": 31, \"label\": \"sweets\"}, {\"from\": 30, \"to\": 41, \"label\": \"toffee\"}, {\"from\": 30, \"to\": 7, \"label\": \"chocolate\"}, {\"from\": 30, \"to\": 1, \"label\": \"chocolate\"}, {\"from\": 30, \"to\": 39, \"label\": \"sweets\"}, {\"from\": 30, \"to\": 33, \"label\": \"sweets\"}, {\"from\": 34, \"to\": 1, \"label\": \"chocolate\"}, {\"from\": 13, \"to\": 20, \"label\": \"chocolate\"}, {\"from\": 13, \"to\": 6, \"label\": \"chocolate\"}, {\"from\": 5, \"to\": 38, \"label\": \"chocolate\"}, {\"from\": 25, \"to\": 21, \"label\": \"chocolate\"}, {\"from\": 25, \"to\": 5, \"label\": \"chocolate\"}, {\"from\": 25, \"to\": 7, \"label\": \"chocolate\"}, {\"from\": 25, \"to\": 15, \"label\": \"sweets\"}, {\"from\": 17, \"to\": 23, \"label\": \"toffee\"}, {\"from\": 17, \"to\": 6, \"label\": \"toffee\"}, {\"from\": 17, \"to\": 35, \"label\": \"chocolate\"}, {\"from\": 17, \"to\": 39, \"label\": \"chocolate\"}, {\"from\": 17, \"to\": 20, \"label\": \"chocolate\"}, {\"from\": 17, \"to\": 11, \"label\": \"sweets\"}, {\"from\": 6, \"to\": 33, \"label\": \"toffee\"}, {\"from\": 6, \"to\": 23, \"label\": \"toffee\"}, {\"from\": 6, \"to\": 28, \"label\": \"chocolate\"}, {\"from\": 6, \"to\": 3, \"label\": \"chocolate\"}, {\"from\": 6, \"to\": 31, \"label\": \"chocolate\"}, {\"from\": 6, \"to\": 21, \"label\": \"sweets\"}, {\"from\": 6, \"to\": 20, \"label\": \"sweets\"}, {\"from\": 19, \"to\": 20, \"label\": \"chocolate\"}, {\"from\": 19, \"to\": 24, \"label\": \"chocolate\"}, {\"from\": 19, \"to\": 13, \"label\": \"chocolate\"}, {\"from\": 36, \"to\": 4, \"label\": \"chocolate\"}, {\"from\": 36, \"to\": 32, \"label\": \"chocolate\"}, {\"from\": 9, \"to\": 25, \"label\": \"chocolate\"}, {\"from\": 27, \"to\": 16, \"label\": \"chocolate\"}, {\"from\": 27, \"to\": 8, \"label\": \"chocolate\"}, {\"from\": 27, \"to\": 15, \"label\": \"sweets\"}, {\"from\": 27, \"to\": 33, \"label\": \"sweets\"}, {\"from\": 27, \"to\": 32, \"label\": \"sweets\"}, {\"from\": 35, \"to\": 18, \"label\": \"chocolate\"}, {\"from\": 35, \"to\": 27, \"label\": \"chocolate\"}, {\"from\": 35, \"to\": 14, \"label\": \"chocolate\"}, {\"from\": 33, \"to\": 15, \"label\": \"toffee\"}, {\"from\": 33, \"to\": 0, \"label\": \"chocolate\"}, {\"from\": 33, \"to\": 19, \"label\": \"chocolate\"}, {\"from\": 33, \"to\": 27, \"label\": \"sweets\"}, {\"from\": 33, \"to\": 21, \"label\": \"sweets\"}, {\"from\": 4, \"to\": 20, \"label\": \"chocolate\"}, {\"from\": 4, \"to\": 0, \"label\": \"chocolate\"}, {\"from\": 39, \"to\": 27, \"label\": \"chocolate\"}, {\"from\": 39, \"to\": 11, \"label\": \"chocolate\"}, {\"from\": 39, \"to\": 38, \"label\": \"chocolate\"}, {\"from\": 39, \"to\": 11, \"label\": \"sweets\"}, {\"from\": 20, \"to\": 19, \"label\": \"chocolate\"}, {\"from\": 20, \"to\": 39, \"label\": \"sweets\"}, {\"from\": 20, \"to\": 25, \"label\": \"sweets\"}, {\"from\": 20, \"to\": 31, \"label\": \"sweets\"}, {\"from\": 20, \"to\": 6, \"label\": \"sweets\"}, {\"from\": 41, \"to\": 30, \"label\": \"toffee\"}, {\"from\": 41, \"to\": 24, \"label\": \"chocolate\"}, {\"from\": 41, \"to\": 32, \"label\": \"chocolate\"}, {\"from\": 41, \"to\": 33, \"label\": \"sweets\"}, {\"from\": 41, \"to\": 30, \"label\": \"sweets\"}, {\"from\": 41, \"to\": 4, \"label\": \"sweets\"}, {\"from\": 24, \"to\": 11, \"label\": \"chocolate\"}, {\"from\": 24, \"to\": 37, \"label\": \"chocolate\"}, {\"from\": 24, \"to\": 31, \"label\": \"chocolate\"}, {\"from\": 14, \"to\": 36, \"label\": \"chocolate\"}, {\"from\": 14, \"to\": 11, \"label\": \"chocolate\"}, {\"from\": 14, \"to\": 31, \"label\": \"chocolate\"}, {\"from\": 14, \"to\": 22, \"label\": \"chocolate\"}, {\"from\": 29, \"to\": 38, \"label\": \"chocolate\"}, {\"from\": 29, \"to\": 2, \"label\": \"chocolate\"}];\n",
       "\n",
       "    var container = document.getElementById(\"e00a60a4-a11c-48fd-9c55-d3bf5dc72e2b\");\n",
       "\n",
       "    var data = {\n",
       "        nodes: nodes,\n",
       "        edges: edges\n",
       "    };\n",
       "\n",
       "    var options = {\n",
       "    nodes: {\n",
       "        shape: 'dot',\n",
       "        size: 25,\n",
       "        font: {\n",
       "            size: 14\n",
       "        }\n",
       "    },\n",
       "    edges: {\n",
       "        font: {\n",
       "            size: 14,\n",
       "            align: 'middle'\n",
       "        },\n",
       "        color: 'gray',\n",
       "        arrows: {\n",
       "            to: {\n",
       "                enabled: true,\n",
       "                scaleFactor: 0.5\n",
       "            }\n",
       "        },\n",
       "        smooth: {\n",
       "            enabled: false\n",
       "        }\n",
       "    },\n",
       "    physics: {\n",
       "        enabled: true\n",
       "        }\n",
       "    };\n",
       "\n",
       "    var network = new vis.Network(container, data, options);\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Plot with neo4jupyter\n",
    "import neo4jupyter\n",
    "neo4jupyter.init_notebook_mode()\n",
    "\n",
    "neo4jupyter.draw(graph, {\"User\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_nodes_from_edge_tables(graph_specification,\n",
    "                                          spark_context,\n",
    "                                          input_edge_path,\n",
    "                                          output_node_id_col,\n",
    "                                          output_label_col,\n",
    "                                          output_tag_col,\n",
    "                                          data_format='parquet',\n",
    "                                          array_delimiter=';'):\n",
    "    sql_context = HiveContext(spark_context)\n",
    "\n",
    "    for edge_kind in graph_specification.edge_lists:\n",
    "        data = (sql_context\n",
    "                .read.format(data_format)\n",
    "                .option('header', 'true')\n",
    "                .option('inferschema', 'true')\n",
    "                .load(os.path.join(input_edge_path, edge_kind.safe_name)))\n",
    "        \n",
    "        nodes_concat = union_all([\n",
    "            data.select(col(edge_kind.source_column.safe_name).alias(output_node_id_col)),\n",
    "            data.select(col(edge_kind.target_column.safe_name).alias(output_node_id_col))\n",
    "        ])\n",
    "\n",
    "        transformed = (\n",
    "            nodes_concat\n",
    "            .withColumn(output_label_col, lit(None))\n",
    "            .withColumn(output_tag_col, lit(None))\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        yield transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second part?\n",
    "# if 'neo4j_writer' in dolist:\n",
    "#     # This part inserts the remainder of node properties that were not captured above\n",
    "#     neo4j_writer.write_neo4j_nodes(graph_specification=spec,\n",
    "#                                    spark_config=SparkConfFactory()\n",
    "#                                    .set_master('local[*]')\n",
    "#                                    .set_app_name('write neo4j nodes')\n",
    "#                                    .set('spark.executor.memory',\n",
    "#                                         '1g')\n",
    "#                                    )\n",
    "\n",
    "#     datetime_now = datetime.now()\n",
    "#     logging.info(\"Backing up db, then purge it...\")\n",
    "#     neo4j_manager.backup(graph_spec, datetime_now,\n",
    "#                          username=neo4j_ssh_username,\n",
    "#                          port=neo4j_ssh_port)\n",
    "#     neo4j_manager.purge(graph_spec,\n",
    "#                         username=neo4j_ssh_username,\n",
    "#                         port=neo4j_ssh_port)\n",
    "#     logging.info(\"Restoring the backup to db...\")\n",
    "#     neo4j_manager.restore(graph_spec,\n",
    "#                           datetime_now,\n",
    "#                           username=neo4j_ssh_username,\n",
    "#                           port=neo4j_ssh_port)\n",
    "\n",
    "#     logging.info(\"Checking write neo4j...\")\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "#             assert test_neo4j_writer(\n",
    "#                 spark_ctx, sql_context, neo_context, spec\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'graph_tools' in dolist:\n",
    "#     # Test graph_construction_coi.get_graph_dataframes\n",
    "#     data_path = os.environ['PIPELINE_DATA_PATH']\n",
    "#     graph_name = graph_spec.name\n",
    "#     node_path_resolved = os.path.join(data_path, graph_name, 'node_list_resolved')\n",
    "#     edge_path_resolved = os.path.join(data_path, graph_name, 'edge_list_resolved')\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         graph = get_graph_dataframes(graph_spec, sql_context,\n",
    "#                                      node_path_resolved, edge_path_resolved,\n",
    "#                                      DATA_FORMAT)\n",
    "\n",
    "#         assert 'node_list' in graph\n",
    "#         assert 'edge_list' in graph\n",
    "#         assert len(graph['node_list']) == len(graph_spec.node_lists)\n",
    "#         for cur_node_list in graph_spec.node_lists:\n",
    "#             assert cur_node_list.safe_name in graph['node_list']\n",
    "#         assert len(graph['edge_list']) == len(graph_spec.edge_lists)\n",
    "#         for cur_edge_list in graph_spec.edge_lists:\n",
    "#             assert cur_edge_list.safe_name in graph['edge_list']\n",
    "\n",
    "#     # Test graph_construction_coi.data_loading\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         tables = load_node_edge_lists(sql_context, graph_spec,\n",
    "#                                       node_path_resolved, edge_path_resolved,\n",
    "#                                       DATA_FORMAT)\n",
    "#         for cur_edge_list in graph_spec.edge_lists:\n",
    "#             assert (cur_edge_list.safe_table_name,\n",
    "#                     cur_edge_list.source_column.safe_name,\n",
    "#                     cur_edge_list.target_column.safe_name) in tables\n",
    "#         assert len(tables) == len(graph_spec.node_lists) + len(graph_spec.edge_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
