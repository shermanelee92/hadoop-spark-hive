{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run fullchain tests\n",
    "\n",
    "See: finnet-pipeline/docker-tests/fullchain/run_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add packages, append to `dags/requirements_py3.txt` and run `!pip3 install -r /usr/local/dags/requirements_py3.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r /usr/local/dags/requirements_py3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "### Stop current SC, test assumes no existing SC\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GRAPH_DB\"] = \"\"\"bolt://neo4j:test@neo4j:7687\"\"\"\n",
    "os.environ[\"NEO4J_SSH_PORT\"] = \"22\"\n",
    "os.environ[\"NEO4J_SSH_USERNAME\"] = \"root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PIPELINE_DATA_PATH'] = \"/datasets/finnet\"\n",
    "os.environ['PIPELINE_DATA_FORMAT'] = \"parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/dags\")\n",
    "\n",
    "from run_tests import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the list of tasks to test\n",
    "dolist = [\n",
    "    'build_lists', 'resolve_entities',\n",
    "    'neo4j_purger', 'neo4j_writer',\n",
    "    'graph_tools'\n",
    "]\n",
    "\n",
    "# Get neo4j ssh username and port\n",
    "neo4j_ssh_username = os.environ.get('NEO4J_SSH_USERNAME', 'neo4j')\n",
    "neo4j_ssh_port = int(os.environ.get('NEO4J_SSH_PORT', 9000))\n",
    "\n",
    "# Setup the spark configuration\n",
    "config = dict()\n",
    "config['SparkConfiguration'] = (SparkConf()\n",
    "                                .setMaster('local[*]')\n",
    "                                .setAppName(\"test create data\")\n",
    "                                .set(\"spark.executor.memory\", \"1024m\"))\n",
    "\n",
    "# Get the graph specs\n",
    "datalist = os.listdir(LOCAL_DATA_PATH)\n",
    "jsonlist = [k for k in datalist if re.match(r'.*\\.json$', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1 json\n",
    "gspec = jsonlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph spec\n",
    "with open(os.path.join(LOCAL_DATA_PATH, gspec), 'r') as f:\n",
    "    graph_spec = GraphSpec.from_dict(json.load(f))\n",
    "    spec = graph_spec.to_dict()\n",
    "\n",
    "tables_path = os.path.join(DATA_PATH, graph_spec.name, 'tables')\n",
    "# n_path = os.path.join(DATA_PATH, graph_spec.name, 'node_list')\n",
    "# e_path = os.path.join(DATA_PATH, graph_spec.name, 'edge_list')\n",
    "# n_path_res = os.path.join(DATA_PATH, graph_spec.name, 'node_list_resolved')\n",
    "# e_path_res = os.path.join(DATA_PATH, graph_spec.name, 'edge_list_resolved')\n",
    "\n",
    "logging.info(\"Processing \" + gspec)\n",
    "\n",
    "# Use graph specification's neo4j connection\n",
    "neo_config = {\n",
    "    'uri': spec['graph_uri'],\n",
    "    'max_retries': config.get('neo4j.max_retries', 5),\n",
    "    'max_batchsize': config.get('neo4j.max_batchsize', 10000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec\n",
    "\n",
    "# \"\"\"\n",
    "# Note:\n",
    "# test_data_chocolate_node_list: index_column: hidden: True is ignored as intended\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity resolution:\n",
    "\n",
    "How should we disambiguate `S1234567G:Person` and `S1234567G:SoleProprietor`. Need to generate canonical ID? Should be done before hive ingestion then... not part as graph building...? Let them be separate nodes but with 0 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purge existing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purging the graph\n",
    "if 'neo4j_purger' in dolist:\n",
    "    logging.info(\"Purging Neo4j...\")\n",
    "    neo4j_manager.purge(graph_spec,\n",
    "                        username=neo4j_ssh_username,\n",
    "                        port=neo4j_ssh_port)\n",
    "    logging.info(\"Checking purging neo4j...\")\n",
    "    with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "        assert test_neo4j_purger(neo_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 0}]\n"
     ]
    }
   ],
   "source": [
    "# Check nodes in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\"MATCH (n) RETURN count(n) as count\")\n",
    "    print(cursor.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write nodes and edges to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `.': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote temp nodes .csv to /tmp/tmp2fd0pb_u/tmp5okmpeqy.csv\n",
      "Wrote temp edges .csv to /tmp/tmp2fd0pb_u/tmp6u4xx6g5.csv\n",
      "Wrote temp edges .csv to /tmp/tmp2fd0pb_u/tmp2xcoyjti.csv\n",
      "Wrote temp edges .csv to /tmp/tmp2fd0pb_u/tmp6142lk3e.csv\n",
      "Debug mode node and edge count test...\n"
     ]
    }
   ],
   "source": [
    "# Graph writer\n",
    "if 'neo4j_writer' in dolist:\n",
    "    debug_write = True\n",
    "    logging.info(\"Writing to Neo4j...\")\n",
    "        \n",
    "    graph_to_neo4j.graph_to_neo4j(graph_specification=graph_spec,\n",
    "                                  spark_config=SparkConfFactory()\n",
    "                                  .set_master('local[*]')\n",
    "                                  .set_app_name('write neo4j nodes')\n",
    "                                  .set(\"spark.driver.maxResultSize\",\n",
    "                                       \"1g\")\n",
    "                                  .set('spark.executor.memory',\n",
    "                                       '1g'),\n",
    "                                  input_node_path=tables_path,\n",
    "                                  input_edge_path=tables_path,\n",
    "                                  username=neo4j_ssh_username,\n",
    "                                  port=neo4j_ssh_port,\n",
    "                                  debug_write=debug_write,\n",
    "                                  verbose=True\n",
    "                                  )\n",
    "    \n",
    "    if debug_write:\n",
    "        print(\"Debug mode node and edge count test...\")\n",
    "        \n",
    "        node_cnt = 0; edge_cnt=0\n",
    "        debug_dir = \"debug\"\n",
    "        for file in os.listdir(debug_dir):\n",
    "            df = pd.read_csv(os.path.join(debug_dir, file))\n",
    "            cnt = len(df)\n",
    "\n",
    "            if file.startswith(\"node\"):\n",
    "                node_cnt += cnt\n",
    "            elif file.startswith(\"edge\"):\n",
    "                edge_cnt += cnt\n",
    "                \n",
    "        with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "            nc = neo_context.run(\"MATCH (n) RETURN count(n) as count\").data()[0][\"count\"]\n",
    "            assert nc == node_cnt\n",
    "            \n",
    "            ec = neo_context.run(\"MATCH ()-[r]->() RETURN count(r) as count\").data()[0][\"count\"]\n",
    "            assert ec == edge_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect with py2neo\n",
    "from py2neo import Graph\n",
    "graph = Graph(\"bolt://neo4j:test@neo4j:7687\", user=\"neo4j\", password=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 45}]\n"
     ]
    }
   ],
   "source": [
    "# Check nodes in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\"MATCH (n) RETURN count(n) as count\")\n",
    "    print(cursor.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'toffee', '_searchable', '8', 'is_target', 'chocolate', 'sweets'})\n",
      "dict_items([('_canonical_id', '1'), ('edge_metadata', 'foo'), ('sweetness number', '1')])\n"
     ]
    }
   ],
   "source": [
    "# Check node 1 in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\n",
    "        \"\"\"MATCH (n {_canonical_id: \"1\"}) RETURN n\"\"\"\n",
    "    )\n",
    "    \n",
    "    node = cursor.data()[0]['n']\n",
    "    print(node.labels)\n",
    "    assert(\n",
    "        node.labels \n",
    "        == {'chocolate', 'toffee', 'sweets', 'is_target', '8', '_searchable'}\n",
    "    )\n",
    "    \n",
    "    print(node.items())\n",
    "    assert(\n",
    "        set(node.items()) \n",
    "        == {('_canonical_id', '1'), ('edge_metadata', 'foo'), ('sweetness number', '1')}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 244}]\n"
     ]
    }
   ],
   "source": [
    "# Check edges in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\"MATCH ()-[r]-() RETURN count(r) as count\")\n",
    "    print(cursor.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "# Check edges in graph\n",
    "import pandas as pd\n",
    "\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\n",
    "        \"\"\"\n",
    "        MATCH (s)-[r]-(t)\n",
    "        RETURN s._canonical_id as source, t._canonical_id as target\n",
    "        \"\"\")\n",
    "    \n",
    "    edge_table = pd.DataFrame(cursor.data())\n",
    "    print(\n",
    "        len(set(edge_table[\"source\"].tolist() + edge_table[\"target\"].tolist()))\n",
    "    )\n",
    "    print(\n",
    "        len(edge_table.drop_duplicates())\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': '1', 'target': '9'}, {'source': '3', 'target': '19'}, {'source': '7', 'target': '5'}, {'source': '18', 'target': '3'}, {'source': '13', 'target': '29'}, {'source': '14', 'target': '13'}, {'source': '9', 'target': '1'}, {'source': '13', 'target': '14'}, {'source': '19', 'target': '3'}, {'source': '2', 'target': '13'}, {'source': '3', 'target': '18'}, {'source': '6', 'target': '9'}, {'source': '29', 'target': '13'}, {'source': '5', 'target': '7'}, {'source': '13', 'target': '2'}, {'source': '18', 'target': '28'}, {'source': '9', 'target': '6'}, {'source': '28', 'target': '18'}]\n"
     ]
    }
   ],
   "source": [
    "# Check multi-edges in graph\n",
    "import pandas as pd\n",
    "\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\n",
    "        \"\"\"\n",
    "        MATCH (s)-[r]-(t)\n",
    "        WITH s, t, count(r) as rel_cnt\n",
    "        WHERE rel_cnt > 1\n",
    "        RETURN s._canonical_id as source, t._canonical_id as target\n",
    "        \"\"\")\n",
    "    \n",
    "    print(cursor.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'r': <Relationship id=29 nodes=(<Node id=35 labels=set() properties={}>, <Node id=30 labels=set() properties={}>) type='sweets' properties={}>}, {'r': <Relationship id=14 nodes=(<Node id=30 labels=set() properties={}>, <Node id=35 labels=set() properties={}>) type='sweets' properties={}>}]\n"
     ]
    }
   ],
   "source": [
    "# Check multi-edges in graph\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\n",
    "        \"\"\"\n",
    "        MATCH (s)-[r]-(t)\n",
    "        WHERE s._canonical_id='9' AND t._canonical_id='6'\n",
    "        RETURN r\n",
    "        \"\"\")\n",
    "    d = cursor.data()\n",
    "    print(d)\n",
    "    \n",
    "    assert len(d) == 2\n",
    "    \n",
    "    # there exists a 6->9 edge and 9<-6 edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'r': <Relationship id=40 nodes=(<Node id=15 labels=set() properties={}>, <Node id=34 labels=set() properties={}>) type='b;f;toffee' properties={'edge_prop_friendly': 'ep2'}>}]\n"
     ]
    }
   ],
   "source": [
    "# Check edges with extra information\n",
    "with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "    cursor = neo_context.run(\n",
    "        \"\"\"\n",
    "        MATCH (s)-[r]-(t)\n",
    "        WHERE s._canonical_id='1' AND t._canonical_id='0'\n",
    "        RETURN r\n",
    "        \"\"\")\n",
    "    d = cursor.data()\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Connect with py2neo\n",
    "# from py2neo import Graph\n",
    "# graph = Graph(\"bolt://neo4j:test@neo4j:7687\", user=\"neo4j\", password=\"test\")\n",
    "\n",
    "# ### Plot with neo4jupyter\n",
    "# import neo4jupyter\n",
    "# neo4jupyter.init_notebook_mode()\n",
    "\n",
    "# neo4jupyter.draw(graph, {\"User\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lists - Should deprecate this too???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build list\n",
    "# if 'build_lists' in dolist:\n",
    "#     logging.info(\"Building lists...\")\n",
    "#     build_node_lists(\n",
    "#         graph_specification=graph_spec,\n",
    "#         spark_config=(SparkConfFactory()\n",
    "#                       .set_master('local[*]')\n",
    "#                       .set_app_name('test create data')\n",
    "#                       .set('spark.executor.memory', '1g')),\n",
    "#         tables_path=tables_path,\n",
    "#         node_path=n_path,\n",
    "#         data_format=DATA_FORMAT,\n",
    "#     )\n",
    "#     build_edge_lists(\n",
    "#         graph_specification=graph_spec,\n",
    "#         spark_config=(SparkConfFactory()\n",
    "#                       .set_master('local[*]')\n",
    "#                       .set_app_name('test create data')\n",
    "#                       .set('spark.executor.memory', '1g')),\n",
    "#         tables_path=tables_path,\n",
    "#         edge_path=e_path,\n",
    "#         data_format=DATA_FORMAT,\n",
    "#     )\n",
    "#     logging.info(\"Checking build_lists...\")\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         assert test_build_lists(spark_ctx, sql_context, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Reads only needed columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "# with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#     sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#     sql_context.read.parquet(\"/datasets/finnet/test_data/node_list/fn_chocolate_nodes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Reads only needed columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "# with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#     sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#     sql_context.read.parquet(\"/datasets/finnet/test_data/edge_list/fn_toffee_relations\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~Resolve Entities~ Drop... resolve entities to be done before persisting to hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets drop entity res module in this form, any entity res should be done at point of injest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resolve entities\n",
    "# if 'resolve_entities' in dolist:\n",
    "#     logging.info(\"Resolving entities...\")\n",
    "#     resolve_node_entities(\n",
    "#         graph_specification=graph_spec,\n",
    "#         spark_config=(SparkConfFactory()\n",
    "#                       .set_master('local[*]')\n",
    "#                       .set_app_name('test create data')\n",
    "#                       .set('spark.executor.memory', '1g')),\n",
    "#         entity_maps=dict(),\n",
    "#         input_node_path=n_path,\n",
    "#         output_node_path=n_path_res,\n",
    "#         output_node_id='_canonical_id',\n",
    "#         data_format=DATA_FORMAT\n",
    "#     )\n",
    "#     resolve_edge_entities(\n",
    "#         graph_specification=graph_spec,\n",
    "#         spark_config=(SparkConfFactory()\n",
    "#                       .set_master('local[*]')\n",
    "#                       .set_app_name('test create data')\n",
    "#                       .set('spark.executor.memory', '1g')),\n",
    "#         entity_maps=dict(),\n",
    "#         input_edge_path=e_path,\n",
    "#         output_edge_path=e_path_res,\n",
    "#         output_edge_source_id='_canonical_id_source',\n",
    "#         output_edge_target_id='_canonical_id_target',\n",
    "#         data_format=DATA_FORMAT\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Produces _canonical_id columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "# with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#     sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#     sql_context.read.parquet(\"/datasets/finnet/test_data/node_list_resolved/fn_toffee_nodes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Produces _canonical_id columns and writes to HDFS, 1 file per nodekind and edgekind respectively\n",
    "\n",
    "# with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#     sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#     sql_context.read.parquet(\"/datasets/finnet/test_data/edge_list_resolved/fn_toffee_relations\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What does EntityMapper do???\n",
    "# with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#     from fncore.tasks.resolve_entities import EntityMapper\n",
    "    \n",
    "#     entityMapper = EntityMapper(spark_ctx, {})\n",
    "#     # Think these are just placeholder column names\n",
    "#     print(entityMapper._from)\n",
    "#     print(entityMapper._to)\n",
    "#     entityMapper._map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like EntityMapper isn't doing anything here,\n",
    "# since entity_map param is empty, _canonical_id just takes the id from index_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Connect with py2neo\n",
    "# from py2neo import Graph\n",
    "# graph = Graph(\"bolt://neo4j:test@neo4j:7687\", user=\"neo4j\", password=\"test\")\n",
    "\n",
    "# ### Plot with neo4jupyter\n",
    "# import neo4jupyter\n",
    "# neo4jupyter.init_notebook_mode()\n",
    "\n",
    "# neo4jupyter.draw(graph, {\"User\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second part?\n",
    "# if 'neo4j_writer' in dolist:\n",
    "#     # This part inserts the remainder of node properties that were not captured above\n",
    "#     neo4j_writer.write_neo4j_nodes(graph_specification=spec,\n",
    "#                                    spark_config=SparkConfFactory()\n",
    "#                                    .set_master('local[*]')\n",
    "#                                    .set_app_name('write neo4j nodes')\n",
    "#                                    .set('spark.executor.memory',\n",
    "#                                         '1g')\n",
    "#                                    )\n",
    "\n",
    "#     datetime_now = datetime.now()\n",
    "#     logging.info(\"Backing up db, then purge it...\")\n",
    "#     neo4j_manager.backup(graph_spec, datetime_now,\n",
    "#                          username=neo4j_ssh_username,\n",
    "#                          port=neo4j_ssh_port)\n",
    "#     neo4j_manager.purge(graph_spec,\n",
    "#                         username=neo4j_ssh_username,\n",
    "#                         port=neo4j_ssh_port)\n",
    "#     logging.info(\"Restoring the backup to db...\")\n",
    "#     neo4j_manager.restore(graph_spec,\n",
    "#                           datetime_now,\n",
    "#                           username=neo4j_ssh_username,\n",
    "#                           port=neo4j_ssh_port)\n",
    "\n",
    "#     logging.info(\"Checking write neo4j...\")\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         with get_neo4j_context(neo_config['uri']) as neo_context:\n",
    "#             assert test_neo4j_writer(\n",
    "#                 spark_ctx, sql_context, neo_context, spec\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'graph_tools' in dolist:\n",
    "#     # Test graph_construction_coi.get_graph_dataframes\n",
    "#     data_path = os.environ['PIPELINE_DATA_PATH']\n",
    "#     graph_name = graph_spec.name\n",
    "#     node_path_resolved = os.path.join(data_path, graph_name, 'node_list_resolved')\n",
    "#     edge_path_resolved = os.path.join(data_path, graph_name, 'edge_list_resolved')\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         graph = get_graph_dataframes(graph_spec, sql_context,\n",
    "#                                      node_path_resolved, edge_path_resolved,\n",
    "#                                      DATA_FORMAT)\n",
    "\n",
    "#         assert 'node_list' in graph\n",
    "#         assert 'edge_list' in graph\n",
    "#         assert len(graph['node_list']) == len(graph_spec.node_lists)\n",
    "#         for cur_node_list in graph_spec.node_lists:\n",
    "#             assert cur_node_list.safe_name in graph['node_list']\n",
    "#         assert len(graph['edge_list']) == len(graph_spec.edge_lists)\n",
    "#         for cur_edge_list in graph_spec.edge_lists:\n",
    "#             assert cur_edge_list.safe_name in graph['edge_list']\n",
    "\n",
    "#     # Test graph_construction_coi.data_loading\n",
    "#     with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "#         sql_context = SQLContext(spark_ctx, sparkSession=SparkSession(spark_ctx))\n",
    "#         tables = load_node_edge_lists(sql_context, graph_spec,\n",
    "#                                       node_path_resolved, edge_path_resolved,\n",
    "#                                       DATA_FORMAT)\n",
    "#         for cur_edge_list in graph_spec.edge_lists:\n",
    "#             assert (cur_edge_list.safe_table_name,\n",
    "#                     cur_edge_list.source_column.safe_name,\n",
    "#                     cur_edge_list.target_column.safe_name) in tables\n",
    "#         assert len(tables) == len(graph_spec.node_lists) + len(graph_spec.edge_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
