{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for fullchain tests\n",
    "\n",
    "See: finnet-pipeline/docker-tests/fullchain/create_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add packages, append to `dags/requirements_py3.txt` and run `!pip3 install -r /usr/local/dags/requirements_py3.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install -r /usr/local/dags/requirements_py3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "### Stop current SC, test assumes no existing SC\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/dags\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "from fncore_py3.utils.graph_specification import GraphSpec\n",
    "from fncore_py3.utils.spark_tools import get_spark_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/datasets/finnet\"\n",
    "DATA_FORMAT = \"parquet\"\n",
    "LOCAL_DATA_PATH = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "config = dict()\n",
    "config['SparkConfiguration'] = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    ")\n",
    "\n",
    "hiveFolderSep = \"__\"\n",
    "hiveDBName = \"default\"\n",
    "\n",
    "os.environ[\"GRAPH_DB\"] = \"\"\"bolt://neo4j:test@neo4j:7687\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get graph specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /datasets/finnet\r\n"
     ]
    }
   ],
   "source": [
    "# Purge folder\n",
    "!hdfs dfs -rm -r $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.listdir(LOCAL_DATA_PATH)\n",
    "json_list = [k for k in data_list if re.match(r'.*\\.json$', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with get_spark_context(config['SparkConfiguration']) as spark_ctx:\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    # Read in the graph spec\n",
    "    for graph_spec in json_list:\n",
    "        with open(os.path.join(LOCAL_DATA_PATH, graph_spec), 'r') as f:\n",
    "            spec = json.load(f)\n",
    "            spec_model = GraphSpec.from_dict(spec)\n",
    "\n",
    "        tables = spec_model.table_details\n",
    "        graph_name = spec_model.name\n",
    "\n",
    "        # Read the sample data and put into hdfs\n",
    "        for table, columns in tables['tables'].items():\n",
    "            source_table, safe_table = table\n",
    "            filepath = 'file://' + \\\n",
    "                       os.path.join(LOCAL_DATA_PATH, str(source_table)) + \\\n",
    "                       '.csv'\n",
    "            data = spark.read.format('com.databricks.spark.csv')\\\n",
    "                              .option('header', 'true')\\\n",
    "                              .option('inferschema', 'false')\\\n",
    "                              .load(filepath)\n",
    "            \n",
    "            outdatapath = os.path.join(\n",
    "                DATA_PATH, graph_name, 'tables', safe_table\n",
    "            )\n",
    "            data.write.format(DATA_FORMAT)\\\n",
    "                .mode(saveMode='overwrite')\\\n",
    "                .save(outdatapath)\n",
    "\n",
    "#             hivetablename = \".\".join([hiveDBName,hiveFolderSep.join([graph_name, 'tables', safe_table])])\n",
    "#             print(\"Writing {}\".format(hivetablename))\n",
    "#             data.write.format(DATA_FORMAT).mode(\"overwrite\").saveAsTable(hivetablename)\n",
    "            \n",
    "#     spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<fncore_py3.utils.graph_specification.EdgeListSpec at 0x7ff25d629c88>,\n",
       " <fncore_py3.utils.graph_specification.EdgeListSpec at 0x7ff25d6340f0>,\n",
       " <fncore_py3.utils.graph_specification.EdgeListSpec at 0x7ff25d634518>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_model.edge_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_kind = spec_model.edge_lists[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toffee_s',\n",
       " [],\n",
       " [<fncore_py3.utils.graph_specification.ColumnSpec at 0x7ff25d634b38>])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(edge_kind.source_column.name, edge_kind.source_labels, edge_kind.source_metadata_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fn_src_meta'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_kind.source_metadata_columns[0].safe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toffee_t', ['is_target'], [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(edge_kind.target_column.name, edge_kind.target_labels, edge_kind.target_metadata_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'connection': 'data_uri_value',\n",
       " 'poll_frequency': '0 2 * * *',\n",
       " 'tables': {('test_data_chocolate_node_list',\n",
       "   'fn_test_data_chocolate_node_list'): {('id', 'fn_id')},\n",
       "  ('test_data_sweets_node_list',\n",
       "   'fn_test_data_sweets_node_list'): {('id', 'fn_id'), ('prop', 'fn_prop')},\n",
       "  ('test_data_toffee_node_list',\n",
       "   'fn_test_data_toffee_node_list'): {('hide', 'fn_hide'),\n",
       "   ('id', 'fn_id'),\n",
       "   ('prop', 'fn_prop')},\n",
       "  ('test_data_chocolate_edge_list',\n",
       "   'fn_test_data_chocolate_edge_list'): {('chocolate_s',\n",
       "    'fn_chocolate_s'), ('chocolate_t', 'fn_chocolate_t')},\n",
       "  ('test_data_sweets_edge_list',\n",
       "   'fn_test_data_sweets_edge_list'): {('sweets_s', 'fn_sweets_s'), ('sweets_t',\n",
       "    'fn_sweets_t')},\n",
       "  ('test_data_toffee_edge_list',\n",
       "   'fn_test_data_toffee_edge_list'): {('edge_label',\n",
       "    'fn_edge_label'), ('edge_prop', 'fn_edge_prop'), ('src_meta',\n",
       "    'fn_src_meta'), ('toffee_s', 'fn_toffee_s'), ('toffee_t', 'fn_toffee_t')}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_model.table_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_name': 'test_data_chocolate_node_list',\n",
       " 'name': 'chocolate nodes',\n",
       " 'labels': ['chocolate'],\n",
       " 'index_column': {'resolution_alias': 'chocolate',\n",
       "  'variable_definition': 'String',\n",
       "  'name': 'id',\n",
       "  'unrecognized-key': 'value',\n",
       "  'hidden': 'True'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"hidden\" field in json is ignored as designed\n",
    "assert{spec_model.node_lists[0].index_column.hidden == False}\n",
    "spec[\"node_lists\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_model.edge_lists[0].merge_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnet-pipeline (Python 3)",
   "language": "python",
   "name": "finnet-pipeline_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
